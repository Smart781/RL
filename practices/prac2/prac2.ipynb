{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2aA14kYCYSwo"
      },
      "source": [
        "## Динамическое программирование"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "roQ4hAstjVce"
      },
      "source": [
        "Рассмотрим алгоритм итерации по оценкам состояния $V$ (Value Iteration):\n",
        "$$\n",
        "V_{(i+1)}(s) = \\max_a \\sum_{s'} P(s' | s,a) \\cdot [ r(s,a,s') + \\gamma V_{i}(s')]\n",
        "$$\n",
        "На основе оценки $V_i$ можно посчитать функцию оценки $Q_i$ действия $a$ в состоянии $s$:\n",
        "$$\n",
        "Q_i(s, a) = \\sum_{s'} P(s' | s,a) \\cdot [ r(s,a,s') + \\gamma V_{i}(s')]\n",
        "$$\n",
        "$$\n",
        "V_{(i+1)}(s) = \\max_a Q_i(s,a)\n",
        "$$\n",
        "\n",
        "Зададим напрямую модель MDP с картинки:\n",
        "<img src=\"https://raw.githubusercontent.com/Tviskaron/mipt/master/2019/RL/02/mdp.png\" caption=\"Марковский процесс принятия решений\" style=\"width: 400px;\">"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l6LwgNvgYXIP"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "    import google.colab\n",
        "    COLAB = True\n",
        "except ModuleNotFoundError:\n",
        "    COLAB = False\n",
        "    pass\n",
        "\n",
        "import urllib.request\n",
        "import os\n",
        "\n",
        "if not os.path.exists('mdp.py'):\n",
        "    url = 'https://raw.githubusercontent.com/Tviskaron/mipt/master/2019/RL/02/mdp.py'\n",
        "    urllib.request.urlretrieve(url, 'mdp.py')\n",
        "    print(\"mdp.py downloaded successfully\")\n",
        "else:\n",
        "    print(\"mdp.py already exists\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "OpKyGJEJYYDn"
      },
      "outputs": [
        {
          "ename": "TypeError",
          "evalue": "data type 'float128' not understood",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 20\u001b[39m\n\u001b[32m      1\u001b[39m transition_probs = {\n\u001b[32m      2\u001b[39m   \u001b[33m'\u001b[39m\u001b[33ms0\u001b[39m\u001b[33m'\u001b[39m:{\n\u001b[32m      3\u001b[39m     \u001b[33m'\u001b[39m\u001b[33ma0\u001b[39m\u001b[33m'\u001b[39m: {\u001b[33m'\u001b[39m\u001b[33ms0\u001b[39m\u001b[33m'\u001b[39m: \u001b[32m0.5\u001b[39m, \u001b[33m'\u001b[39m\u001b[33ms2\u001b[39m\u001b[33m'\u001b[39m: \u001b[32m0.5\u001b[39m},\n\u001b[32m   (...)\u001b[39m\u001b[32m     13\u001b[39m   }\n\u001b[32m     14\u001b[39m }\n\u001b[32m     15\u001b[39m rewards = {\n\u001b[32m     16\u001b[39m   \u001b[33m'\u001b[39m\u001b[33ms1\u001b[39m\u001b[33m'\u001b[39m: {\u001b[33m'\u001b[39m\u001b[33ma0\u001b[39m\u001b[33m'\u001b[39m: {\u001b[33m'\u001b[39m\u001b[33ms0\u001b[39m\u001b[33m'\u001b[39m: +\u001b[32m5\u001b[39m}},\n\u001b[32m     17\u001b[39m   \u001b[33m'\u001b[39m\u001b[33ms2\u001b[39m\u001b[33m'\u001b[39m: {\u001b[33m'\u001b[39m\u001b[33ma1\u001b[39m\u001b[33m'\u001b[39m: {\u001b[33m'\u001b[39m\u001b[33ms0\u001b[39m\u001b[33m'\u001b[39m: -\u001b[32m1\u001b[39m}}\n\u001b[32m     18\u001b[39m }\n\u001b[32m---> \u001b[39m\u001b[32m20\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmdp\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m MDP\n\u001b[32m     21\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m     22\u001b[39m mdp = MDP(transition_probs, rewards, initial_state=\u001b[33m'\u001b[39m\u001b[33ms0\u001b[39m\u001b[33m'\u001b[39m)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/cursor/RL/.conda/lib/python3.11/site-packages/mdp/__init__.py:131\u001b[39m\n\u001b[32m    127\u001b[39m (numx_description, numx, numx_linalg, numx_fft,\n\u001b[32m    128\u001b[39m  numx_rand, numx_version) = configuration.get_numx()\n\u001b[32m    130\u001b[39m \u001b[38;5;66;03m# import the utils module (used by other modules)\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m131\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m utils\n\u001b[32m    132\u001b[39m \u001b[38;5;66;03m# set symeig\u001b[39;00m\n\u001b[32m    133\u001b[39m utils.symeig = configuration.get_symeig(numx_linalg)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/cursor/RL/.conda/lib/python3.11/site-packages/mdp/utils/__init__.py:6\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mbuiltins\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;28mstr\u001b[39m\n\u001b[32m      4\u001b[39m __docformat__ = \u001b[33m\"\u001b[39m\u001b[33mrestructuredtext en\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mroutines\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (timediff, refcast, scast, rotate, random_rot,\n\u001b[32m      7\u001b[39m                        permute, symrand, norm2, cov2,\n\u001b[32m      8\u001b[39m                        mult_diag, comb, sqrtm, get_dtypes, nongeneral_svd,\n\u001b[32m      9\u001b[39m                        hermitian, cov_maxima,\n\u001b[32m     10\u001b[39m                        lrep, rrep, irep, orthogonal_permutations,\n\u001b[32m     11\u001b[39m                        izip_stretched,\n\u001b[32m     12\u001b[39m                        weighted_choice, bool_to_sign, sign_to_bool, gabor,\n\u001b[32m     13\u001b[39m                        invert_exp_funcs2, inspect_formatargspec)\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     15\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mcollections\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m OrderedDict\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/cursor/RL/.conda/lib/python3.11/site-packages/mdp/utils/routines.py:295\u001b[39m\n\u001b[32m    291\u001b[39m             \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[32m    292\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m types\n\u001b[32m--> \u001b[39m\u001b[32m295\u001b[39m _UNSAFE_DTYPES = \u001b[43m[\u001b[49m\u001b[43mnumx\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m(\u001b[49m\u001b[43md\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43md\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\n\u001b[32m    296\u001b[39m \u001b[43m                  \u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mfloat16\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mfloat128\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mcomplex256\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m    299\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mnongeneral_svd\u001b[39m(A, \u001b[38;5;28mrange\u001b[39m=\u001b[38;5;28;01mNone\u001b[39;00m, **kwargs):\n\u001b[32m    300\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"SVD routine for simple eigenvalue problem, API is compatible with\u001b[39;00m\n\u001b[32m    301\u001b[39m \u001b[33;03m    symeig.\"\"\"\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/cursor/RL/.conda/lib/python3.11/site-packages/mdp/utils/routines.py:295\u001b[39m, in \u001b[36m<listcomp>\u001b[39m\u001b[34m(.0)\u001b[39m\n\u001b[32m    291\u001b[39m             \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[32m    292\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m types\n\u001b[32m--> \u001b[39m\u001b[32m295\u001b[39m _UNSAFE_DTYPES = [\u001b[43mnumx\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m(\u001b[49m\u001b[43md\u001b[49m\u001b[43m)\u001b[49m.type \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m\n\u001b[32m    296\u001b[39m                   [\u001b[33m'\u001b[39m\u001b[33mfloat16\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mfloat128\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mcomplex256\u001b[39m\u001b[33m'\u001b[39m]]\n\u001b[32m    299\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mnongeneral_svd\u001b[39m(A, \u001b[38;5;28mrange\u001b[39m=\u001b[38;5;28;01mNone\u001b[39;00m, **kwargs):\n\u001b[32m    300\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"SVD routine for simple eigenvalue problem, API is compatible with\u001b[39;00m\n\u001b[32m    301\u001b[39m \u001b[33;03m    symeig.\"\"\"\u001b[39;00m\n",
            "\u001b[31mTypeError\u001b[39m: data type 'float128' not understood"
          ]
        }
      ],
      "source": [
        "transition_probs = {\n",
        "  's0':{\n",
        "    'a0': {'s0': 0.5, 's2': 0.5},\n",
        "    'a1': {'s2': 1}\n",
        "  },\n",
        "  's1':{\n",
        "    'a0': {'s0': 0.7, 's1': 0.1, 's2': 0.2},\n",
        "    'a1': {'s1': 0.95, 's2': 0.05}\n",
        "  },\n",
        "  's2':{\n",
        "    'a0': {'s0': 0.4, 's2': 0.6},\n",
        "    'a1': {'s0': 0.3, 's1': 0.3, 's2':0.4}\n",
        "  }\n",
        "}\n",
        "rewards = {\n",
        "  's1': {'a0': {'s0': +5}},\n",
        "  's2': {'a1': {'s0': -1}}\n",
        "}\n",
        "\n",
        "from mdp import MDP\n",
        "import numpy as np\n",
        "mdp = MDP(transition_probs, rewards, initial_state='s0')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IVSC6KXuYcsh"
      },
      "source": [
        "Теперь мы можем использовать это MDP, как и любое другое gym окружение:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PzLyFJ4iYfro"
      },
      "outputs": [],
      "source": [
        "state = mdp.reset()\n",
        "print('initial state =', state)\n",
        "next_state, reward, done, info = mdp.step('a1')\n",
        "print(f'next_state ={next_state}, reward = {reward}, done = {done}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UgRdVPJlYjZ4"
      },
      "source": [
        ":Также, помимо стандартных методов, есть дополнительные, которые пригодятся нам для реализации метода итерации по полезностям."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4zK1xXedYn21"
      },
      "outputs": [],
      "source": [
        "print(\"all_states =\", mdp.get_all_states())\n",
        "print(\"possible_actions('s1') = \", mdp.get_possible_actions('s1'))\n",
        "print(\"next_states('s1', 'a0') = \", mdp.get_next_states('s1', 'a0'))\n",
        "print(\"reward('s1', 'a0', 's0') = \",mdp.get_reward('s1', 'a0', 's0'))\n",
        "print(\"transition_prob('s1', 'a0', 's0') = \",\n",
        "      mdp.get_transition_prob('s1', 'a0', 's0'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Oe_RzZtYq11"
      },
      "source": [
        "### Задание 1\n",
        "\n",
        "Теперь реализуем алгоритм итерации по полезностям, чтобы решить этот вручную заданный MDP. Псевдокод алгоритма:\n",
        "\n",
        "---\n",
        "\n",
        "`1.` Инициализируем $V^{(0)}(s)=0$, для всех $s$\n",
        "\n",
        "`2.` For $i=0, 1, 2, \\dots$\n",
        "\n",
        "`3.` $ \\quad V_{(i+1)}(s) = \\max_a \\sum_{s'} P(s' | s,a) \\cdot [ r(s,a,s') + \\gamma V_{i}(s')]$, для всех $s$\n",
        "\n",
        "---\n",
        "\n",
        "Вначале вычисляем оценку состояния-действия:\n",
        "$$Q_i(s, a) = \\sum_{s'} P(s' | s,a) \\cdot [ r(s,a,s') + \\gamma V_{i}(s')]$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1aA0DQccjody"
      },
      "source": [
        "### 1 балл"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qt0o0MokYv0F"
      },
      "outputs": [],
      "source": [
        "def get_action_value(\n",
        "    mdp, state_values, state, action, gamma\n",
        "):\n",
        "    \"\"\" Вычисляем Q(s,a) по формуле выше \"\"\"\n",
        "    # вычислеяем оценку состояния\n",
        "    # Q =\n",
        "    ####### Здесь ваш код ########\n",
        "    Q = 0.0\n",
        "    for next_state in mdp.get_next_states(state, action):\n",
        "        transition_prob = mdp.get_transition_prob(state, action, next_state)\n",
        "        reward = mdp.get_reward(state, action, next_state)\n",
        "        Q += transition_prob * (reward + gamma * state_values[next_state])\n",
        "    ##############################\n",
        "\n",
        "    return Q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x06WscSIYysp"
      },
      "outputs": [],
      "source": [
        "test_Vs = {s: i for i, s in enumerate(sorted(mdp.get_all_states()))}\n",
        "assert np.isclose(get_action_value(mdp, test_Vs, 's2', 'a1', 0.9), 0.69)\n",
        "assert np.isclose(get_action_value(mdp, test_Vs, 's1', 'a0', 0.9), 3.95)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "87q6GhsMY19h"
      },
      "source": [
        "Теперь оцениваем полезность самого состояния, для этого мы можем использовать предыдущий метод:\n",
        "\n",
        "$$V_{(i+1)}(s) = \\max_a \\sum_{s'} P(s' | s,a) \\cdot [ r(s,a,s') + \\gamma V_{i}(s')] = \\max_a Q_i(s,a)$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2O3QFuoVj1iZ"
      },
      "source": [
        "### 1 балл"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hFqCuRaBY5J_"
      },
      "outputs": [],
      "source": [
        "def get_new_state_value(mdp, state_values, state, gamma):\n",
        "    \"\"\" Считаем следующее V(s) по формуле выше.\"\"\"\n",
        "    if mdp.is_terminal(state):\n",
        "        return 0\n",
        "    # V =\n",
        "    ####### Здесь ваш код ########\n",
        "    possible_actions = mdp.get_possible_actions(state)\n",
        "    Q_values = []\n",
        "    for action in possible_actions:\n",
        "        Q = get_action_value(mdp, state_values, state, action, gamma)\n",
        "        Q_values.append(Q)\n",
        "    V = max(Q_values) if Q_values else 0\n",
        "    ##############################\n",
        "\n",
        "    return V"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lPUyRzQOY8PP"
      },
      "outputs": [],
      "source": [
        "test_Vs_copy = dict(test_Vs)\n",
        "assert np.isclose(get_new_state_value(mdp, test_Vs, 's0', 0.9), 1.8)\n",
        "assert np.isclose(get_new_state_value(mdp, test_Vs, 's2', 0.9), 1.08)\n",
        "assert np.isclose(get_new_state_value(mdp, {'s0': -1e10, 's1': 0, 's2': -2e10}, 's0', 0.9), -13500000000.0), \\\n",
        "   \"Убедитесь, что вы правильно обрабатываете отрицательные значения Q произвольной величины.\"\n",
        "assert test_Vs == test_Vs_copy, \"Убедитесь, что вы не изменяете state_values в функции get_new_state_value\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Od-SBiPKY_Q3"
      },
      "source": [
        "Теперь создаем основной цикл итерационного оценки полезности состояний с критерием остановки, который проверяет насколько изменились полезности."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q-0-PlvmkF7P"
      },
      "source": [
        "### 1 балл"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uUwb5JCDZDD4"
      },
      "outputs": [],
      "source": [
        "def value_iteration(\n",
        "    mdp, state_values=None,\n",
        "    gamma = 0.9, num_iter = 1000, min_difference = 1e-5\n",
        "):\n",
        "    \"\"\" выполняет num_iter шагов итерации по значениям\"\"\"\n",
        "    # инициализируем V(s)\n",
        "    state_values = state_values or \\\n",
        "    {s : 0 for s in mdp.get_all_states()}\n",
        "\n",
        "    for i in range(num_iter):\n",
        "        # Вычисляем новые полезности состояний,\n",
        "        # используя функции, определенные выше.\n",
        "        # Должен получиться словарь {s: new_V(s)}\n",
        "        # new_state_values =\n",
        "        ####### Здесь ваш код ########\n",
        "        new_state_values = {}\n",
        "        for state in mdp.get_all_states():\n",
        "            new_state_values[state] = get_new_state_value(mdp, state_values, state, gamma)\n",
        "        ##############################\n",
        "\n",
        "        assert isinstance(new_state_values, dict)\n",
        "\n",
        "        # Считаем разницу\n",
        "        diff = max(\n",
        "            abs(new_state_values[s] - state_values[s])\n",
        "            for s in mdp.get_all_states()\n",
        "        )\n",
        "\n",
        "        print(\n",
        "            f\"iter {i:4} | diff: {diff:6.5f} \"\n",
        "            f\"| V(start): {new_state_values[mdp._initial_state]:.3f} \"\n",
        "        )\n",
        "\n",
        "        state_values = new_state_values\n",
        "        if diff < min_difference:\n",
        "            print(\"Принято! Алгоритм сходится!\")\n",
        "            break\n",
        "\n",
        "    return state_values\n",
        "\n",
        "state_values = value_iteration(\n",
        "    mdp, num_iter = 100, min_difference = 0.001\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZKomkPSrZGlZ"
      },
      "outputs": [],
      "source": [
        "print(\"Final state values:\", state_values)\n",
        "\n",
        "assert abs(state_values['s0'] - 3.781) < 0.01\n",
        "assert abs(state_values['s1'] - 7.294) < 0.01\n",
        "assert abs(state_values['s2'] - 4.202) < 0.01"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6gz5JxncZJoX"
      },
      "source": [
        "По найденным полезностям и зная модель переходов легко найти оптимальную стратегию:\n",
        "$$\\pi^*(s) = argmax_a \\sum_{s'} P(s' | s,a) \\cdot [ r(s,a,s') + \\gamma V_{i}(s')] = argmax_a Q_i(s,a)$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ml9AWeYrkNgf"
      },
      "source": [
        "### 1 балл"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7gd4m26TZOn3"
      },
      "outputs": [],
      "source": [
        "def get_optimal_action(\n",
        "    mdp, state_values, state, gamma=0.9\n",
        "):\n",
        "    \"\"\" Находим оптимальное действие, используя формулу выше. \"\"\"\n",
        "    if mdp.is_terminal(state): return None\n",
        "\n",
        "    actions = mdp.get_possible_actions(state)\n",
        "    # выбираем лучшее действие\n",
        "    # i =\n",
        "    ####### Здесь ваш код ########\n",
        "    Q_values = []\n",
        "    for action in actions:\n",
        "        Q = get_action_value(mdp, state_values, state, action, gamma)\n",
        "        Q_values.append(Q)\n",
        "    i = np.argmax(Q_values)\n",
        "    ##############################\n",
        "\n",
        "    return actions[i]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yv3MRqaQZSzs"
      },
      "outputs": [],
      "source": [
        "assert get_optimal_action(mdp, state_values, 's0', 0.9) == 'a1'\n",
        "assert get_optimal_action(mdp, state_values, 's1', 0.9) == 'a0'\n",
        "assert get_optimal_action(mdp, state_values, 's2', 0.9) == 'a1'\n",
        "\n",
        "assert get_optimal_action(mdp, {'s0': -1e10, 's1': 0, 's2': -2e10}, 's0', 0.9) == 'a0', \\\n",
        "    \"Убедитесь, что вы правильно обрабатываете отрицательные значения Q произвольной величины.\"\n",
        "assert get_optimal_action(mdp, {'s0': -2e10, 's1': 0, 's2': -1e10}, 's0', 0.9) == 'a1', \\\n",
        "    \"Убедитесь, что вы правильно обрабатываете отрицательные значения Q произвольной величины.\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V1KMZyhbZVVX"
      },
      "outputs": [],
      "source": [
        "# Проверим среднее вознаграждение агента\n",
        "\n",
        "s = mdp.reset()\n",
        "rewards = []\n",
        "for _ in range(10000):\n",
        "    s, r, done, _ = mdp.step(get_optimal_action(mdp, state_values, s, 0.9))\n",
        "    rewards.append(r)\n",
        "\n",
        "print(\"average reward: \", np.mean(rewards))\n",
        "\n",
        "assert(0.40 < np.mean(rewards) < 0.55)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xkokwmulZYYn"
      },
      "source": [
        "### Задание 2\n",
        "\n",
        "Теперь проверим работу итерации по ценностям на классической задаче FrozenLake."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E4V34IMzZbeH"
      },
      "outputs": [],
      "source": [
        "from mdp import FrozenLakeEnv\n",
        "mdp = FrozenLakeEnv(slip_chance=0)\n",
        "\n",
        "mdp.render()\n",
        "state_values = value_iteration(mdp)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vNPtPQo2ZdpV"
      },
      "source": [
        "Визуализируем нашу стратегию."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aQP4HnjNZg4C"
      },
      "outputs": [],
      "source": [
        "def draw_policy(mdp, state_values, gamma=0.9):\n",
        "    \"\"\"функция визуализации стратегии\"\"\"\n",
        "    plt.figure(figsize=(3, 3))\n",
        "    h, w = mdp.desc.shape\n",
        "    states = sorted(mdp.get_all_states())\n",
        "    V = np.array([state_values[s] for s in states])\n",
        "    Pi = {\n",
        "        s: get_optimal_action(mdp, state_values, s, gamma)\n",
        "        for s in states\n",
        "    }\n",
        "    plt.imshow(\n",
        "        V.reshape(w, h),\n",
        "        cmap='gray', interpolation='none',\n",
        "        clim=(0, 1)\n",
        "    )\n",
        "    ax = plt.gca()\n",
        "    ax.set_xticks(np.arange(h) - .5)\n",
        "    ax.set_yticks(np.arange(w) - .5)\n",
        "    ax.set_xticklabels([])\n",
        "    ax.set_yticklabels([])\n",
        "    Y, X = np.mgrid[0:4, 0:4]\n",
        "    a2uv = {'left': (-1, 0), 'down': (0, -1),\n",
        "            'right': (1, 0), 'up': (-1, 0)}\n",
        "    for y in range(h):\n",
        "        for x in range(w):\n",
        "            plt.text(x, y, str(mdp.desc[y, x].item()),\n",
        "                     color='g', size=12,\n",
        "                     verticalalignment='center',\n",
        "                     horizontalalignment='center',\n",
        "                     fontweight='bold')\n",
        "            a = Pi[y, x]\n",
        "            if a is None: continue\n",
        "            u, v = a2uv[a]\n",
        "            plt.arrow(x, y, u * .3, -v * .3,\n",
        "                      color='m', head_width=0.1,\n",
        "                      head_length=0.1)\n",
        "    plt.grid(color='b', lw=2, ls='-')\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UJ2zkkx2Zlec"
      },
      "outputs": [],
      "source": [
        "from IPython.display import clear_output\n",
        "from time import sleep\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "mdp = FrozenLakeEnv(map_name='8x8',slip_chance=0.1)\n",
        "state_values = {s : 0 for s in mdp.get_all_states()}\n",
        "\n",
        "for i in range(30):\n",
        "    clear_output(True)\n",
        "    print(\"after iteration %i\"%i)\n",
        "    state_values = value_iteration(mdp,\n",
        "                            state_values, num_iter=1)\n",
        "    draw_policy(mdp, state_values)\n",
        "    sleep(0.5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nltqOBDfZoFG"
      },
      "source": [
        "Посмотрим на оптимальную стратегию:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CKJ1oJapZq77"
      },
      "outputs": [],
      "source": [
        "s = mdp.reset()\n",
        "mdp.render()\n",
        "for t in range(100):\n",
        "    a = get_optimal_action(mdp, state_values, s, 0.9)\n",
        "    print(a, end='\\n\\n')\n",
        "    s, r, done, _ = mdp.step(a)\n",
        "    mdp.render()\n",
        "    if done:\n",
        "        break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ksq-NonlZtHM"
      },
      "source": [
        "Тестируем на более сложном варианте окружения:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z6g4fbKkkZza"
      },
      "source": [
        "### 1 балл"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yOBqWNBfZv6v"
      },
      "outputs": [],
      "source": [
        "mdp = FrozenLakeEnv(slip_chance=0.2, map_name='8x8')\n",
        "state_values = value_iteration(mdp)\n",
        "\n",
        "total_rewards = []\n",
        "for game_i in range(1000):\n",
        "    s = mdp.reset()\n",
        "    rewards = []\n",
        "    for t in range(100):\n",
        "        # выполняем оптимальное действие в окружении\n",
        "        ####### Здесь ваш код ########\n",
        "        a = get_optimal_action(mdp, state_values, s, 0.9)\n",
        "        s, r, done, _ = mdp.step(a)\n",
        "        ##############################\n",
        "\n",
        "        rewards.append(r)\n",
        "        if done:\n",
        "            break\n",
        "    total_rewards.append(np.sum(rewards))\n",
        "\n",
        "print(\"Cреднее вознаграждение:\", np.mean(total_rewards))\n",
        "assert(0.6 <= np.mean(total_rewards) <= 0.8)\n",
        "print(\"Принято!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sOEo-OheZyYP"
      },
      "source": [
        "### Задание 3\n",
        "\n",
        "Теперь рассмотрим алгоритм итерации по стратегиям (PI, policy iteration):\n",
        "\n",
        "---\n",
        "Initialize $\\pi_0$   `// случайно`\n",
        "\n",
        "For $n=0, 1, 2, \\dots$\n",
        "- Считаем функцию $V^{\\pi_{n}}$\n",
        "- Используя $V^{\\pi_{n}}$, считаем функцию $Q^{\\pi_{n}}$\n",
        "- Получаем новую стратегию: $\\pi_{n+1}(s) = \\operatorname*{argmax}_a Q^{\\pi_{n}}(s,a)$\n",
        "---\n",
        "\n",
        "PI включает в себя оценку полезности состояния, как внутренний шаг."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Diaeh1f7Z010"
      },
      "source": [
        "Вначале оценим полезности, используя текущую стратегию:\n",
        "$$V^{\\pi}(s) = \\sum_{s'} P(s,\\pi(s),s')[ R(s,\\pi(s),s') + \\gamma V^{\\pi}(s')]$$\n",
        "    Мы будем искать точное решение, хотя могли использовать и предыдущий итерационный подход. Для этого будем решать систему линейных уравнений относительно $V^{\\pi}(s_i)$ с помощью np.linalg.solve."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3pFDjkE2kfsY"
      },
      "source": [
        "### 3 балла"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-RpV4Yw8Z3bi"
      },
      "outputs": [],
      "source": [
        "from numpy.linalg import solve\n",
        "\n",
        "def compute_vpi(mdp, policy, gamma):\n",
        "    \"\"\"\n",
        "    Считем V^pi(s) для всех состояний, согласно стратегии.\n",
        "    :param policy: словарь состояние->действие {s : a}\n",
        "    :returns: словарь {state : V^pi(state)}\n",
        "    \"\"\"\n",
        "    states = mdp.get_all_states()\n",
        "    A, b = [], []\n",
        "    for i, state in enumerate(states):\n",
        "        if state in policy:\n",
        "            a = policy[state]\n",
        "            # формируем матрицу A (... A.append(...))\n",
        "            ####### Здесь ваш код ########\n",
        "            row = [0] * len(states)\n",
        "            row[i] = 1.0\n",
        "            for next_state in mdp.get_next_states(state, a):\n",
        "                prob = mdp.get_transition_prob(state, a, next_state)\n",
        "                j = states.index(next_state)\n",
        "                row[j] -= gamma * prob\n",
        "            A.append(row)\n",
        "            ##############################\n",
        "\n",
        "            # и вектор b (b.append(...))\n",
        "            ####### Здесь ваш код ########\n",
        "            b_val = 0.0\n",
        "            for next_state in mdp.get_next_states(state, a):\n",
        "                prob = mdp.get_transition_prob(state, a, next_state)\n",
        "                reward = mdp.get_reward(state, a, next_state)\n",
        "                b_val += prob * reward\n",
        "            b.append(b_val)\n",
        "            ##############################\n",
        "\n",
        "        else:\n",
        "            # формируем матрицу A (... A.append(...))\n",
        "            ####### Здесь ваш код ########\n",
        "            row = [0] * len(states)\n",
        "            row[i] = 1.0\n",
        "            A.append(row)\n",
        "            ##############################\n",
        "\n",
        "            # вектор b (b.append(...))\n",
        "            ####### Здесь ваш код ########\n",
        "            b.append(0.0)\n",
        "            ##############################\n",
        "\n",
        "    A = np.array(A)\n",
        "    b = np.array(b)\n",
        "    values = solve(A, b)\n",
        "\n",
        "    state_values = {\n",
        "        states[i] : values[i]\n",
        "        for i in range(len(states))\n",
        "    }\n",
        "    return state_values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qeb79E20Z6d7"
      },
      "outputs": [],
      "source": [
        "transition_probs = {\n",
        "    's0': {\n",
        "        'a0': {'s0': 0.5, 's2': 0.5},\n",
        "        'a1': {'s2': 1}\n",
        "    },\n",
        "    's1': {\n",
        "        'a0': {'s0': 0.7, 's1': 0.1, 's2': 0.2},\n",
        "        'a1': {'s1': 0.95, 's2': 0.05}\n",
        "    },\n",
        "    's2': {\n",
        "        'a0': {'s0': 0.4, 's1': 0.6},\n",
        "        'a1': {'s0': 0.3, 's1': 0.3, 's2': 0.4}\n",
        "    }\n",
        "}\n",
        "rewards = {\n",
        "    's1': {'a0': {'s0': +5}},\n",
        "    's2': {'a1': {'s0': -1}}\n",
        "}\n",
        "mdp = MDP(transition_probs, rewards, initial_state='s0')\n",
        "\n",
        "gamma = 0.9\n",
        "\n",
        "test_policy = {\n",
        "    s: np.random.choice(mdp.get_possible_actions(s))\n",
        "    for s in mdp.get_all_states()}\n",
        "new_vpi = compute_vpi(mdp, test_policy, gamma)\n",
        "\n",
        "print(new_vpi)\n",
        "assert type(new_vpi) is dict, \\\n",
        "    \"функция compute_vpi должна возвращать словарь \\\n",
        "    {состояние s : V^pi(s) }\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Du2YNXpxZ9BT"
      },
      "source": [
        "Теперь обновляем стратегию на основе новых значений полезностей:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WsYlHPblkrSI"
      },
      "source": [
        "### 1 балл"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TGCHMeaUZ_Qj"
      },
      "outputs": [],
      "source": [
        "def compute_new_policy(mdp, vpi, gamma):\n",
        "    \"\"\"\n",
        "    Рассчитываем новую стратегию\n",
        "    :param vpi: словарь {state : V^pi(state) }\n",
        "    :returns: словарь {state : оптимальное действие}\n",
        "    \"\"\"\n",
        "    Q = {}\n",
        "    for state in mdp.get_all_states():\n",
        "        Q[state] = {}\n",
        "        for a in mdp.get_possible_actions(state):\n",
        "            values = []\n",
        "            for next_state in mdp.get_next_states(state, a):\n",
        "                r = mdp.get_reward(state, a, next_state)\n",
        "                p = mdp.get_transition_prob(\n",
        "                    state, a, next_state\n",
        "                )\n",
        "                # values.append(...)\n",
        "                ####### Здесь ваш код ########\n",
        "                values.append(p * (r + gamma * vpi[next_state]))\n",
        "                ##############################\n",
        "\n",
        "            Q[state][a] = sum(values)\n",
        "\n",
        "    policy = {}\n",
        "    for state in mdp.get_all_states():\n",
        "        actions = mdp.get_possible_actions(state)\n",
        "        if actions:\n",
        "            # выбираем оптимальное действие в state\n",
        "            # policy[state] = ...\n",
        "            ####### Здесь ваш код ########\n",
        "            best_action = max(actions, key=lambda a: Q[state][a])\n",
        "            policy[state] = best_action\n",
        "            ##############################\n",
        "\n",
        "    return policy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1b1OXlg9aBsy"
      },
      "outputs": [],
      "source": [
        "new_policy = compute_new_policy(mdp, new_vpi, gamma)\n",
        "\n",
        "print(new_policy)\n",
        "\n",
        "assert type(new_policy) is dict, \\\n",
        "\"функция compute_new_policy должна возвращать словарь \\\n",
        "{состояние s: оптимальное действие}\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "15mJglOZaEmI"
      },
      "source": [
        "Собираем все в единый цикл:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cIrgcIqKkxD2"
      },
      "source": [
        "### 1 балл"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2LcLHHhIaHAZ"
      },
      "outputs": [],
      "source": [
        "def policy_iteration(\n",
        "    mdp, policy=None, gamma = 0.9,\n",
        "    num_iter = 1000, min_difference = 1e-5\n",
        "):\n",
        "    \"\"\"\n",
        "    Запускаем цикл итерации по стратегиям\n",
        "    Если стратегия не определена, задаем случайную\n",
        "    \"\"\"\n",
        "    for i in range(num_iter):\n",
        "        if not policy:\n",
        "            policy = {}\n",
        "            for s in mdp.get_all_states():\n",
        "                if mdp.get_possible_actions(s):\n",
        "                    policy[s] = (\n",
        "                        np.random.choice(mdp.get_possible_actions(s))\n",
        "                    )\n",
        "        # state_values =\n",
        "        ####### Здесь ваш код ########\n",
        "        state_values = compute_vpi(mdp, policy, gamma)\n",
        "        ##############################\n",
        "\n",
        "        #policy =\n",
        "        ####### Здесь ваш код ########\n",
        "        new_policy = compute_new_policy(mdp, state_values, gamma)\n",
        "        \n",
        "        # Проверяем сходимость\n",
        "        policy_changed = False\n",
        "        for s in mdp.get_all_states():\n",
        "            if s in policy and s in new_policy:\n",
        "                if policy[s] != new_policy[s]:\n",
        "                    policy_changed = True\n",
        "                    break\n",
        "            elif (s in policy) != (s in new_policy):\n",
        "                policy_changed = True\n",
        "                break\n",
        "        \n",
        "        policy = new_policy\n",
        "        if not policy_changed:\n",
        "            print(\"Принято! Стратегия сходится!\")\n",
        "            break\n",
        "        ##############################\n",
        "    return state_values, policy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ddfLTSfgaJjU"
      },
      "source": [
        "Тестируем на FrozenLake."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4hLv3X0OaKmg"
      },
      "outputs": [],
      "source": [
        "mdp = FrozenLakeEnv(slip_chance=0.1)\n",
        "state_values, policy = policy_iteration(mdp)\n",
        "\n",
        "total_rewards = []\n",
        "for game_i in range(1000):\n",
        "    s = mdp.reset()\n",
        "    rewards = []\n",
        "    for t in range(100):\n",
        "        s, r, done, _ = mdp.step(policy[s])\n",
        "        rewards.append(r)\n",
        "        if done:\n",
        "            break\n",
        "    total_rewards.append(np.sum(rewards))\n",
        "\n",
        "print(\"average reward: \", np.mean(total_rewards))\n",
        "assert(0.8 <= np.mean(total_rewards) <= 0.95)\n",
        "print(\"Принято!\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
