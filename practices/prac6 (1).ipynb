{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ByKY8Rp-qjuj"
      },
      "source": [
        "# Actor-Critic\n",
        "\n",
        "Теорема о градиенте стратегии связывает градиент целевой функции  и градиент самой стратегии:\n",
        "\n",
        "$$\\nabla_\\theta J(\\theta) = \\mathbb{E}_\\pi [Q^\\pi(s, a) \\nabla_\\theta \\ln \\pi_\\theta(a \\vert s)]$$\n",
        "\n",
        "Встает вопрос, как оценить $Q^\\pi(s, a)$? В чистом policy-based алгоритме REINFORCE используется отдача $G_t$, полученная методом Монте-Карло в качестве несмещенной оценки $Q^\\pi(s, a)$. В Actor-Critic же предлагается отдельно обучать нейронную сеть Q-функции — критика.\n",
        "\n",
        "Актор-критиком часто называют обобщенный фреймворк (подход), нежели какой-то конкретный алгоритм. Как подход актор-критик не указывает, каким конкретно [policy gradient] методом обучается актор и каким [value based] методом обучается критик. Таким образом актор-критик задает целое [семейство](https://proceedings.neurips.cc/paper_files/paper/1999/file/6449f44a102fde848669bdd9eb6b76fa-Paper.pdf) различных алгоритмов. Рекомендую в качестве шпаргалки использовать упомянутый в тетрадке с REINFORCE [пост из блога Lilian Weng](https://lilianweng.github.io/posts/2018-04-08-policy-gradient/), посвященный наиболее популярным алгоритмам семейства актор-критиков\n",
        "\n",
        "В данной тетрадке познакомимся с наиболее простым вариантом актор-критика, который так и называют Actor-Critic:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "823sXK2eqyte",
        "outputId": "6b4e8902-0099-4f59-af1e-ce40f98664cd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33mWARNING: gymnasium 1.2.2 does not provide the extra 'accept-rom-license'\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "# Cтавим нужные зависимости, если это колаб\n",
        "try:\n",
        "    import google.colab\n",
        "    COLAB = True\n",
        "except ModuleNotFoundError:\n",
        "    COLAB = False\n",
        "    pass\n",
        "\n",
        "if COLAB:\n",
        "    !pip -q install \"gymnasium[classic-control, atari, accept-rom-license]\"\n",
        "    !pip -q install piglet\n",
        "    !pip -q install imageio_ffmpeg\n",
        "    !pip -q install moviepy==1.0.3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "MRq9NksDrpw7"
      },
      "outputs": [],
      "source": [
        "from collections import deque\n",
        "\n",
        "import gymnasium as gym\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.distributions import Categorical\n",
        "\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k8jg4GH8r0zr",
        "outputId": "d8d74d39-039d-4388-b6fb-37396b510ac6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "env.observation_space=Box([-4.8               -inf -0.41887903        -inf], [4.8               inf 0.41887903        inf], (4,), float32)\n",
            "env.action_space=Discrete(2)\n",
            "Action_space: 2 | State_space: (4,)\n"
          ]
        }
      ],
      "source": [
        "env = gym.make(\"CartPole-v1\")\n",
        "env.reset()\n",
        "\n",
        "print(f'{env.observation_space=}')\n",
        "print(f'{env.action_space=}')\n",
        "\n",
        "n_actions = env.action_space.n\n",
        "state_dim = env.observation_space.shape\n",
        "print(f'Action_space: {n_actions} | State_space: {env.observation_space.shape}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t0udnb6ARBTl"
      },
      "source": [
        "(1 балл)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "2ItOnbBmsDrV"
      },
      "outputs": [],
      "source": [
        "def to_tensor(x, dtype=np.float32):\n",
        "    if isinstance(x, torch.Tensor):\n",
        "        return x\n",
        "    x = np.asarray(x, dtype=dtype)\n",
        "    x = torch.from_numpy(x)\n",
        "    return x\n",
        "\n",
        "def symlog(x):\n",
        "    \"\"\"Compute symlog values for a vector `x`. It's an inverse operation for symexp.\"\"\"\n",
        "    return torch.sign(x) * torch.log(torch.abs(x) + 1)\n",
        "\n",
        "def symexp(x):\n",
        "    \"\"\"Compute symexp values for a vector `x`. It's an inverse operation for symlog.\"\"\"\n",
        "    return torch.sign(x) * (torch.exp(torch.abs(x)) - 1.0)\n",
        "\n",
        "\n",
        "class SymExpModule(nn.Module):\n",
        "    def forward(self, x):\n",
        "        return symexp(x)\n",
        "\n",
        "def select_action_eps_greedy(Q, state, epsilon):\n",
        "    \"\"\"Выбирает действие epsilon-жадно.\"\"\"\n",
        "    if not isinstance(state, torch.Tensor):\n",
        "        state = torch.tensor(state, dtype=torch.float32)\n",
        "    Q_s = Q(state).detach().numpy()\n",
        "\n",
        "    if np.random.random() < epsilon:\n",
        "        action = np.random.randint(len(Q_s))\n",
        "    else:\n",
        "        action = np.argmax(Q_s)\n",
        "\n",
        "    action = int(action)\n",
        "    return action\n",
        "\n",
        "def sample_batch(replay_buffer, n_samples):\n",
        "    # sample randomly `n_samples` samples from replay buffer\n",
        "    # and split an array of samples into arrays: states, actions, rewards, next_actions, terminateds\n",
        "    if len(replay_buffer) < n_samples:\n",
        "        n_samples = len(replay_buffer)\n",
        "\n",
        "    indices = np.random.choice(len(replay_buffer), n_samples, replace=False)\n",
        "    batch = [replay_buffer[i] for i in indices]\n",
        "\n",
        "    states = [experience[0] for experience in batch]\n",
        "    actions = [experience[1] for experience in batch]\n",
        "    rewards = [experience[2] for experience in batch]\n",
        "    next_states = [experience[3] for experience in batch]\n",
        "    terminateds = [experience[4] for experience in batch]\n",
        "\n",
        "    return np.array(states), np.array(actions), np.array(rewards), np.array(next_states), np.array(terminateds)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RNMK_gF6sRO-"
      },
      "source": [
        "## Shared-body Actor-Critic\n",
        "\n",
        "Актор и критик могут обучаться в разных режимах — актор только on-policy (шаг обучения на текущей собранной подтраектории), а критик on-policy или off-policy (шаг обучения на текущей подтраектории или на батче из replay buffer). Это с одной стороны привносит гибкость в обучение, с другой — усложняет его.\n",
        "\n",
        "Если актор и критик оба обучаются on-policy, то имеет смысл объединить их сетки в одну и делать общий шаг обратного распространения ошибки. Однако, если они обучаются в разных режимах (и с разной частотой обновления), то велика вероятность, что их шаги обучения могут начать конфликтовать в случае общего тела — для такого варианта намного предпочтительнее разделить их на разные подсети (либо аккуратно настраивать гиперпарметры, чтобы стабилизировать обучение). В целом, рекомендуется использовать общий энкодер наблюдений, а далее как можно скорее разделять головы.\n",
        "\n",
        "Сделаем реализацию актор-критика с общим телом и с on-policy вариантом обучения."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "kvvLaw3Lsd9Y"
      },
      "outputs": [],
      "source": [
        "class ActorBatch:\n",
        "    def __init__(self):\n",
        "        self.logprobs = []\n",
        "        self.q_values = []\n",
        "\n",
        "    def append(self, log_prob, q_value):\n",
        "        self.logprobs.append(log_prob)\n",
        "        self.q_values.append(q_value)\n",
        "\n",
        "    def clear(self):\n",
        "        self.logprobs.clear()\n",
        "        self.q_values.clear()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DoTv-E6rRTfH"
      },
      "source": [
        "(3 балла)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "qtVGcaHUtSCO"
      },
      "outputs": [],
      "source": [
        "class ActorCriticModel(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dims, output_dim):\n",
        "        super().__init__()\n",
        "        self.output_dim = output_dim\n",
        "\n",
        "        # Исправляем обработку input_dim\n",
        "        if isinstance(input_dim, tuple):\n",
        "            input_size = input_dim[0]\n",
        "        else:\n",
        "            input_size = input_dim\n",
        "\n",
        "        layers = []\n",
        "        prev_dim = input_size\n",
        "        for hidden_dim in hidden_dims:\n",
        "            layers.append(nn.Linear(prev_dim, hidden_dim))\n",
        "            layers.append(nn.ReLU())\n",
        "            prev_dim = hidden_dim\n",
        "\n",
        "        self.net = nn.Sequential(*layers)\n",
        "\n",
        "        self.actor_head = nn.Sequential(\n",
        "            nn.Linear(prev_dim, output_dim),\n",
        "            nn.Softmax(dim=-1)\n",
        "        )\n",
        "\n",
        "        self.critic_head = nn.Linear(prev_dim, output_dim)\n",
        "\n",
        "    def forward(self, state):\n",
        "        if not isinstance(state, torch.Tensor):\n",
        "            state = torch.FloatTensor(state)\n",
        "\n",
        "        # Обрабатываем как одиночное состояние, так и батч\n",
        "        if len(state.shape) == 1:\n",
        "            state = state.unsqueeze(0)\n",
        "\n",
        "        features = self.net(state)\n",
        "        action_probs = self.actor_head(features)\n",
        "\n",
        "        dist = Categorical(action_probs)\n",
        "        action = dist.sample()\n",
        "        log_prob = dist.log_prob(action)\n",
        "\n",
        "        q_values = self.critic_head(features)\n",
        "        Q_s_a = q_values.gather(1, action.unsqueeze(-1)).squeeze(-1)\n",
        "\n",
        "        return action, log_prob, Q_s_a\n",
        "\n",
        "    def evaluate(self, state):\n",
        "        if not isinstance(state, torch.Tensor):\n",
        "            state = torch.FloatTensor(state)\n",
        "\n",
        "        if len(state.shape) == 1:\n",
        "            state = state.unsqueeze(0)\n",
        "\n",
        "        features = self.net(state)\n",
        "        q_values = self.critic_head(features)\n",
        "        return q_values"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nco1WpaORXsY"
      },
      "source": [
        "(6 баллов)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "QhPhsYPBtZPg"
      },
      "outputs": [],
      "source": [
        "class ActorCriticAgent:\n",
        "    def __init__(self, state_dim, action_dim, hidden_dims, lr, gamma, critic_rb_size):\n",
        "        self.lr = lr\n",
        "        self.gamma = gamma\n",
        "\n",
        "        self.actor_critic = ActorCriticModel(state_dim, hidden_dims, action_dim)\n",
        "        self.opt = torch.optim.Adam(self.actor_critic.parameters(), lr=lr)\n",
        "\n",
        "        self.actor_batch = ActorBatch()\n",
        "        self.critic_rb = deque(maxlen=critic_rb_size)\n",
        "\n",
        "    def act(self, state):\n",
        "        # УБИРАЕМ torch.no_grad() - нам нужны градиенты для log_prob\n",
        "        action, log_prob, q_value = self.actor_critic(state)\n",
        "\n",
        "        # q_value детачим, но log_prob оставляем с градиентами\n",
        "        self.actor_batch.append(log_prob, q_value.detach())\n",
        "\n",
        "        return action.item()\n",
        "\n",
        "    def append_to_replay_buffer(self, s, a, r, next_s, terminated):\n",
        "        self.critic_rb.append((s, a, r, next_s, terminated))\n",
        "\n",
        "    def evaluate(self, state):\n",
        "        return self.actor_critic.evaluate(state)\n",
        "\n",
        "    def update(self, rollout_size, critic_batch_size, critic_updates_per_actor):\n",
        "        if len(self.actor_batch.q_values) < rollout_size:\n",
        "            return\n",
        "\n",
        "        self.opt.zero_grad()\n",
        "        critic_loss = self.update_critic(critic_batch_size, critic_updates_per_actor)\n",
        "        actor_loss = self.update_actor()\n",
        "        total_loss = critic_loss + actor_loss\n",
        "\n",
        "        # Проверяем, требует ли loss градиенты\n",
        "        if not total_loss.requires_grad:\n",
        "            print(\"Warning: total_loss doesn't require grad!\")\n",
        "            return\n",
        "\n",
        "        total_loss.backward()\n",
        "        self.opt.step()\n",
        "        self.actor_batch.clear()\n",
        "\n",
        "    def update_actor(self):\n",
        "        Q_s_a = torch.stack(self.actor_batch.q_values)\n",
        "        logprobs = torch.stack(self.actor_batch.logprobs)\n",
        "\n",
        "        # Проверяем, что logprobs требуют градиенты\n",
        "        if not logprobs.requires_grad:\n",
        "            # Если нет, пересчитываем с градиентами\n",
        "            print(\"Warning: logprobs don't require grad, recalculating...\")\n",
        "            states = [exp[0] for exp in self.critic_rb][-len(logprobs):]\n",
        "            states_tensor = torch.FloatTensor(np.array(states))\n",
        "            _, new_logprobs, _ = self.actor_critic(states_tensor)\n",
        "            logprobs = torch.stack(new_logprobs)\n",
        "\n",
        "        # Policy gradient loss\n",
        "        loss = -torch.mean(Q_s_a * logprobs)\n",
        "        return loss\n",
        "\n",
        "    def update_critic(self, batch_size, n_updates=1):\n",
        "        if len(self.critic_rb) < batch_size:\n",
        "            # Возвращаем тензор, который требует градиенты\n",
        "            return torch.tensor(0.0, requires_grad=True)\n",
        "\n",
        "        total_loss = torch.tensor(0.0, requires_grad=True)\n",
        "        for _ in range(n_updates):\n",
        "            states, actions, rewards, next_states, terminateds = sample_batch(\n",
        "                self.critic_rb, batch_size\n",
        "            )\n",
        "\n",
        "            loss = self.compute_td_loss(states, actions, rewards, next_states, terminateds)\n",
        "            total_loss = total_loss + loss\n",
        "\n",
        "        return total_loss / n_updates\n",
        "\n",
        "    def compute_td_loss(self, states, actions, rewards, next_states, terminateds, regularizer=0.1):\n",
        "        s = to_tensor(states)\n",
        "        a = to_tensor(actions, int).long()\n",
        "        r = to_tensor(rewards)\n",
        "        s_next = to_tensor(next_states)\n",
        "        term = to_tensor(terminateds, bool)\n",
        "\n",
        "        # Вычисляем Q-значения - они будут требовать градиенты\n",
        "        q_values = self.actor_critic.evaluate(s)\n",
        "        Q_s_a = q_values.gather(1, a.unsqueeze(1)).squeeze(1)\n",
        "\n",
        "        # Целевые значения вычисляем без градиентов\n",
        "        with torch.no_grad():\n",
        "            q_next = self.actor_critic.evaluate(s_next)\n",
        "            V_sn, _ = torch.max(q_next, dim=1)\n",
        "\n",
        "        target = r + self.gamma * V_sn * (~term).float()\n",
        "        td_error = target - Q_s_a\n",
        "\n",
        "        loss = torch.mean(td_error ** 2)\n",
        "        loss += regularizer * torch.mean(Q_s_a ** 2)\n",
        "        return loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "jAID76g8tg_R",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "18f20ba5-89f1-4d0a-bf68-e94d3b181d1c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "global_step=2002 | avg_return=10.000\n",
            "global_step=4002 | avg_return=10.500\n",
            "global_step=6019 | avg_return=18.667\n",
            "global_step=8023 | avg_return=20.000\n",
            "global_step=10013 | avg_return=22.400\n",
            "global_step=12000 | avg_return=24.833\n",
            "global_step=14042 | avg_return=29.857\n",
            "global_step=16036 | avg_return=37.500\n",
            "global_step=18068 | avg_return=49.889\n",
            "global_step=20000 | avg_return=64.100\n",
            "global_step=22189 | avg_return=84.500\n",
            "global_step=24037 | avg_return=99.100\n",
            "global_step=26125 | avg_return=126.300\n",
            "global_step=28077 | avg_return=133.300\n",
            "global_step=30100 | avg_return=147.500\n",
            "global_step=32052 | avg_return=150.500\n",
            "global_step=34042 | avg_return=167.400\n",
            "global_step=36168 | avg_return=181.700\n",
            "global_step=38013 | avg_return=188.100\n",
            "global_step=40053 | avg_return=182.500\n",
            "global_step=42026 | avg_return=186.300\n",
            "global_step=44033 | avg_return=186.000\n",
            "global_step=46004 | avg_return=173.200\n",
            "global_step=48205 | avg_return=191.000\n",
            "global_step=50168 | avg_return=193.300\n",
            "global_step=52177 | avg_return=205.300\n",
            "Решено!\n"
          ]
        }
      ],
      "source": [
        "def run_actor_critic(\n",
        "        env_name=\"CartPole-v1\",\n",
        "        hidden_dims=(128, 128), lr=5e-4,\n",
        "        total_max_steps=200_000,\n",
        "        train_schedule=16, replay_buffer_size=50000, batch_size=64, critic_updates_per_actor=4,\n",
        "        eval_schedule=1000, smooth_ret_window=10, success_ret=200.\n",
        "):\n",
        "    env = gym.make(env_name)\n",
        "    episode_return_history = deque(maxlen=smooth_ret_window)\n",
        "\n",
        "    agent = ActorCriticAgent(\n",
        "        state_dim=env.observation_space.shape[0], action_dim=env.action_space.n, hidden_dims=hidden_dims,\n",
        "        lr=lr, gamma=.995, critic_rb_size=replay_buffer_size\n",
        "    )\n",
        "\n",
        "    s, _ = env.reset()\n",
        "    done, episode_return = False, 0.\n",
        "    eval = False\n",
        "\n",
        "    for global_step in range(1, total_max_steps+1):\n",
        "        a = agent.act(s)\n",
        "        s_next, r, terminated, truncated, _ = env.step(a)\n",
        "        episode_return += r\n",
        "        done = terminated or truncated\n",
        "\n",
        "        # train step\n",
        "        agent.append_to_replay_buffer(s, a, r, s_next, terminated)\n",
        "        agent.update(train_schedule, batch_size, critic_updates_per_actor)\n",
        "\n",
        "        # evaluate\n",
        "        if global_step % eval_schedule == 0:\n",
        "            eval = True\n",
        "\n",
        "        s = s_next\n",
        "        if done:\n",
        "            if eval:\n",
        "                episode_return_history.append(episode_return)\n",
        "                avg_return = np.mean(episode_return_history)\n",
        "                print(f'{global_step=} | {avg_return=:.3f}')\n",
        "                if avg_return >= success_ret:\n",
        "                    print('Решено!')\n",
        "                    break\n",
        "\n",
        "            s, _ = env.reset()\n",
        "            done, episode_return = False, 0.\n",
        "            eval = False\n",
        "\n",
        "run_actor_critic(eval_schedule=2000, total_max_steps=100_000)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "sqMm2I1IoeDO"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}