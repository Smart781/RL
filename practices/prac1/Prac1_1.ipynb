{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k4NMHBq16Y_g"
      },
      "source": [
        "# RL basics\n",
        "\n",
        "Термины и понятия:\n",
        "\n",
        "- агент/среда\n",
        "- наблюдение $o$ / состояние $s$\n",
        "- действие $a$, стратегия $\\pi: \\pi(s) \\rightarrow a$ функция перехода $T: T(s, a) \\rightarrow s'$\n",
        "- вознаграждение $r$, ф-я вознаграждений $R: R(s, a) \\rightarrow r$\n",
        "- цикл взаимодействия, траектория $\\tau: (s_0, a_0, r_0, s_1, a_1, r_1, ..., s_T, a_T, r_T)$, эпизод\n",
        "- отдача $G$, подсчет отдачи, средняя[/ожидаемая] отдача $\\mathbb{E}[G]$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "MoNP7Wdn6aP0"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "    import google.colab\n",
        "    COLAB = True\n",
        "except ModuleNotFoundError:\n",
        "    COLAB = False\n",
        "    pass\n",
        "\n",
        "if COLAB:\n",
        "    !pip -q install \"gymnasium[classic-control, atari, accept-rom-license]\"\n",
        "    !pip -q install piglet\n",
        "    !pip -q install imageio_ffmpeg\n",
        "    !pip -q install moviepy==1.0.3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "9JPaLF5v6esZ"
      },
      "outputs": [],
      "source": [
        "import glob\n",
        "import io\n",
        "import base64\n",
        "import gymnasium as gym\n",
        "import numpy as np\n",
        "from IPython import display as ipythondisplay\n",
        "from IPython.display import HTML\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OGAoJeNF6hJO"
      },
      "source": [
        "## Agent, environment\n",
        "\n",
        "<img src=https://gymnasium.farama.org/_images/lunar_lander.gif caption=\"lunar lander\" width=\"150\" height=\"50\"><img src=https://gymnasium.farama.org/_images/mountain_car.gif caption=\"mountain car\" width=\"150\" height=\"50\">\n",
        "<img src=https://gymnasium.farama.org/_images/cliff_walking.gif caption=\"cliff walking\" width=\"300\" height=\"50\">\n",
        "<img src=https://ale.farama.org/_images/montezuma_revenge.gif caption=\"montezuma revenge\" width=\"150\" height=\"100\">\n",
        "<img src=https://github.com/danijar/crafter/raw/main/media/video.gif caption=\"crafter\" width=\"150\" height=\"100\">\n",
        "<img src=https://camo.githubusercontent.com/6df2ca438d8fe8aa7a132b859315147818c54af608f8609320c3c20e938acf48/68747470733a2f2f6d656469612e67697068792e636f6d2f6d656469612f344e78376759694d394e44724d724d616f372f67697068792e676966 caption=\"malmo minecraft\" width=\"150\" height=\"100\">\n",
        "<img src=https://images.ctfassets.net/kftzwdyauwt9/e0c0947f-1a44-4528-4a41450a9f0a/2d0e85871d58d02dbe01b2469d693d4a/table-03.gif caption=\"roboschool\" width=\"150\" height=\"100\">\n",
        "<img src=https://raw.githubusercontent.com/Tviskaron/mipt/master/2019/RL/02/mdp.png caption=\"Марковский процесс принятия решений\" width=\"150\" height=\"100\">\n",
        "<img src=https://minigrid.farama.org/_images/DoorKeyEnv.gif caption=\"minigrid\" width=\"120\" height=\"120\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dcyLKga76mA_"
      },
      "source": [
        "## Observation, state\n",
        "\n",
        "TODO:\n",
        "- добавить примеры наблюдений/состояний (числа, векторы, картинки)\n",
        "- интуитивное объяснение различия, положить пока, что наблюдение = состояние\n",
        "- пространство состояний\n",
        "\n",
        "\n",
        "В каждый момент времени среда имеет некоторое внутреннее состояние. Здесь слово \"состояние\" я употребил скорее в интуитивном понимании, чтобы обозначить, что среда изменчива (иначе какой смысл с ней взаимодействовать, если ничего не меняется). В обучении с подкреплением под термином состояние $s$ (или $s_t$, где $t$ — текущее время) подразумевают либо абстрактно информацию о \"состоянии\" среды, либо ее явное представление в виде данных, достаточные для полного описания \"состояния\". *NB: Здесь можно провести аналогию с компьютерными играми — файл сохранения игры как раз содержит информацию о \"состоянии\" мира игры, чтобы в будущем можно было продолжить с текущей точки, так что данные этого файла в целом можно с некоторой натяжкой считать состоянием (с натяжкой, потому что редко когда в сложных играх файлы сохранения содержат прямо вот всю информацию, так что после перезагрузки вы получите не совсем точную копию). При этом обычно подразумевается, что состояние не содержит в себе ничего лишнего, то есть это **минимальный** набор информации.*\n",
        "\n",
        "Наблюдением $o$ называют то, что агент \"видит\" о текущем состоянии среды. Это не обязательно зрение, а вообще вся доступная ему информация (условно, со всех его органов чувств).\n",
        "\n",
        "В общем случае наблюдение: кортеж/словарь многомерных векторов чисел."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ypHv9w6i6pcX"
      },
      "outputs": [],
      "source": [
        "print(gym.make(\"CartPole-v0\").reset()[0].shape)\n",
        "print(gym.make(\"MountainCar-v0\").reset()[0].shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GabuCLcJ67lb"
      },
      "source": [
        "## Action, policy, transition function\n",
        "\n",
        "Рассмотрим следующие MDP:\n",
        "\n",
        "- A: <img src=https://i.ibb.co/mrCMVZLQ/mdp-a.png caption=\"A\" width=\"400\" height=\"100\">\n",
        "- B: <img src=https://i.ibb.co/GQ2tVtjC/mdp-b.png caption=\"B\" width=\"400\" height=\"100\">\n",
        "\n",
        "Links to all:\n",
        "[A](https://i.ibb.co/mrCMVZLQ/mdp-a.png)\n",
        "[B](https://i.ibb.co/GQ2tVtjC/mdp-b.png)\n",
        "[C](https://i.ibb.co/Jj9LYHjP/mdp-c.png)\n",
        "[D](https://i.ibb.co/Y47Mr83b/mdp-d.png)\n",
        "[E](https://i.ibb.co/Kjt1Xhmf/mdp-e.png)\n",
        "\n",
        "Давайте явно запишем пространства состояний $S$ и действий $A$, а также функцию перехода $T$ среды."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4XpqNc_o6_CS"
      },
      "outputs": [],
      "source": [
        "states = set(range(3))\n",
        "actions = set(range(1))\n",
        "\n",
        "print(f'{states=} | {actions=}')\n",
        "\n",
        "T = {\n",
        "    (0, 0): 1,\n",
        "    (1, 0): 2,\n",
        "    (2, 0): 2\n",
        "}\n",
        "print(f'Transition function {T=}')\n",
        "\n",
        "A_mdp = states, actions, T"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KlqB4WcZ7CDK"
      },
      "source": [
        "Попробуйте записать функцию перехода в матричном виде:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jmg8hnng7EnJ"
      },
      "outputs": [],
      "source": [
        "# Матричная форма функции перехода T\n",
        "# Матрица перехода P[s][s'] = вероятность перехода из состояния s в состояние s' при действии a\n",
        "# В данном случае у нас только одно действие (0), поэтому матрица 3x3\n",
        "\n",
        "# Создаем матрицу перехода для MDP A\n",
        "# Состояния: 0, 1, 2; Действие: 0\n",
        "T_matrix = np.zeros((3, 3))\n",
        "T_matrix[0, 1] = 1.0  # из состояния 0 в состояние 1\n",
        "T_matrix[1, 2] = 1.0  # из состояния 1 в состояние 2\n",
        "T_matrix[2, 2] = 1.0  # из состояния 2 в состояние 2 (терминальное)\n",
        "\n",
        "print(\"Матрица переходов T:\")\n",
        "print(T_matrix)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_FPshg_07G0R"
      },
      "source": [
        "Как получить вероятность нахождения агента в состоянии (1) через N шагов? Что происходит с вероятностями нахождения в состояниях при $N \\rightarrow \\infty$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9pwc4Atn7IAf"
      },
      "outputs": [],
      "source": [
        "# Вероятность нахождения агента в состоянии (1) через N шагов\n",
        "# Начальное состояние: 0\n",
        "initial_state = np.array([1.0, 0.0, 0.0])  # вероятность быть в состоянии 0 = 1.0\n",
        "\n",
        "# Рассмотрим несколько значений N\n",
        "N_values = [1, 2, 3, 5, 10, 20, 50, 100]\n",
        "\n",
        "probabilities_in_state_1 = []\n",
        "\n",
        "for N in N_values:\n",
        "    # Вычисляем распределение вероятностей через N шагов\n",
        "    state_dist = initial_state.copy()\n",
        "    for _ in range(N):\n",
        "        state_dist = state_dist @ T_matrix\n",
        "    \n",
        "    # Вероятность быть в состоянии 1\n",
        "    prob_state_1 = state_dist[1]\n",
        "    probabilities_in_state_1.append(prob_state_1)\n",
        "    print(f\"Через {N} шагов: P(состояние 1) = {prob_state_1:.6f}\")\n",
        "\n",
        "# Что происходит при N -> infinity?\n",
        "# Вычисляем стационарное распределение\n",
        "eigenvalues, eigenvectors = np.linalg.eig(T_matrix.T)\n",
        "# Находим собственный вектор с собственным значением 1 (стационарное распределение)\n",
        "stationary_idx = np.where(np.abs(eigenvalues - 1.0) < 1e-6)[0]\n",
        "if len(stationary_idx) > 0:\n",
        "    stationary = eigenvectors[:, stationary_idx[0]].real\n",
        "    stationary = stationary / stationary.sum()  # нормализуем\n",
        "    print(f\"\\nСтационарное распределение (N -> infinity):\")\n",
        "    print(f\"P(состояние 0) = {stationary[0]:.6f}\")\n",
        "    print(f\"P(состояние 1) = {stationary[1]:.6f}\")\n",
        "    print(f\"P(состояние 2) = {stationary[2]:.6f}\")\n",
        "\n",
        "# Визуализация\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(N_values, probabilities_in_state_1, 'o-')\n",
        "plt.xlabel('N (количество шагов)')\n",
        "plt.ylabel('Вероятность нахождения в состоянии 1')\n",
        "plt.title('Вероятность нахождения агента в состоянии 1 через N шагов')\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p52R04np7Ku0"
      },
      "source": [
        "Задайте еще несколько MDP:\n",
        "\n",
        "- C: <img src=https://i.ibb.co/Jj9LYHjP/mdp-c.png caption=\"C\" width=\"400\" height=\"100\">"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JSdJ9ZsI7Nfw"
      },
      "outputs": [],
      "source": [
        "# MDP C: задаем пространство состояний, действий и функцию перехода\n",
        "# Исходя из диаграммы C (нужно посмотреть на изображение, но предположу структуру)\n",
        "# Обычно в таких задачах C может иметь другую структуру переходов\n",
        "\n",
        "states_C = set(range(3))  # предположим 3 состояния\n",
        "actions_C = set(range(2))  # предположим 2 действия\n",
        "\n",
        "# Задаем функцию перехода для MDP C\n",
        "# (это пример, может потребоваться корректировка в зависимости от реальной диаграммы)\n",
        "T_C = {\n",
        "    (0, 0): 1,  # из состояния 0 при действии 0 -> состояние 1\n",
        "    (0, 1): 2,  # из состояния 0 при действии 1 -> состояние 2\n",
        "    (1, 0): 2,  # из состояния 1 при действии 0 -> состояние 2\n",
        "    (1, 1): 0,  # из состояния 1 при действии 1 -> состояние 0\n",
        "    (2, 0): 2,  # из состояния 2 при действии 0 -> состояние 2 (терминальное)\n",
        "    (2, 1): 2   # из состояния 2 при действии 1 -> состояние 2 (терминальное)\n",
        "}\n",
        "\n",
        "print(f\"MDP C:\")\n",
        "print(f\"Состояния: {states_C}\")\n",
        "print(f\"Действия: {actions_C}\")\n",
        "print(f\"Функция перехода: {T_C}\")\n",
        "\n",
        "C_mdp = states_C, actions_C, T_C\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AiU5X4DH7TaG"
      },
      "source": [
        "Давайте попробуем задать двух агентов: случайного и оптимального (для каждой среды свой)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "It2waXJi7WWN"
      },
      "outputs": [],
      "source": [
        "class Agent:\n",
        "    def __init__(self, actions):\n",
        "        self.rng = np.random.default_rng()\n",
        "        self.actions = np.array(list(actions))\n",
        "\n",
        "    def act(self, state):\n",
        "        return self.rng.integers(len(self.actions))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VH6uo4EP7ZqB"
      },
      "source": [
        "В качестве дополнения, запишите стратегию агента"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t68c_r-W7asH"
      },
      "outputs": [],
      "source": [
        "# Стратегия агента - функция, которая для каждого состояния возвращает действие\n",
        "# Для случайного агента стратегия - равномерное распределение по действиям\n",
        "\n",
        "def policy_random(state, actions):\n",
        "    \"\"\"\n",
        "    Случайная стратегия: равномерное распределение по всем действиям\n",
        "    \"\"\"\n",
        "    return np.random.choice(list(actions))\n",
        "\n",
        "def policy_deterministic(state, actions, policy_dict):\n",
        "    \"\"\"\n",
        "    Детерминированная стратегия: для каждого состояния задано конкретное действие\n",
        "    policy_dict: словарь {состояние: действие}\n",
        "    \"\"\"\n",
        "    return policy_dict.get(state, list(actions)[0])\n",
        "\n",
        "# Пример стратегии для MDP A (только одно действие, поэтому тривиально)\n",
        "policy_A = {\n",
        "    0: 0,\n",
        "    1: 0,\n",
        "    2: 0\n",
        "}\n",
        "\n",
        "print(f\"Стратегия для MDP A: {policy_A}\")\n",
        "\n",
        "# Пример стратегии для MDP C\n",
        "policy_C = {\n",
        "    0: 0,  # в состоянии 0 выбираем действие 0\n",
        "    1: 1,  # в состоянии 1 выбираем действие 1\n",
        "    2: 0   # в состоянии 2 выбираем действие 0\n",
        "}\n",
        "\n",
        "print(f\"Стратегия для MDP C: {policy_C}\")\n",
        "\n",
        "# В вероятностном виде стратегия может быть представлена как:\n",
        "# π(a|s) - вероятность выбора действия a в состоянии s\n",
        "\n",
        "def policy_probabilistic(state, actions, policy_probs):\n",
        "    \"\"\"\n",
        "    Вероятностная стратегия\n",
        "    policy_probs: словарь {состояние: массив вероятностей для каждого действия}\n",
        "    \"\"\"\n",
        "    probs = policy_probs.get(state, np.ones(len(actions)) / len(actions))\n",
        "    return np.random.choice(list(actions), p=probs)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XGx2-KeH7lL4"
      },
      "source": [
        "## Reward, reward function\n",
        "\n",
        "Теперь добавим произвольную функцию вознаграждения. Например, для A:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fk7umEnA7oFv"
      },
      "outputs": [],
      "source": [
        "R = {\n",
        "    (0, 0): -0.1,\n",
        "    (1, 0): 1.0,\n",
        "    (2, 0): 0.0\n",
        "}\n",
        "print(R)\n",
        "\n",
        "A_mdp = *A_mdp, R\n",
        "print(A_mdp)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j92TZ1l67rVh"
      },
      "source": [
        "## Interaction loop, trajectory, termination, truncation, episode\n",
        "\n",
        "Общий цикл взаимодействия в рамках эпизода:\n",
        "1. Инициализировать среду: $s \\leftarrow \\text{env.init()}$\n",
        "2. Цикл:\n",
        "    - выбрать действие: $a \\leftarrow \\pi(s)$\n",
        "    - получить ответ от среды: $s, r, d \\leftarrow \\text{env.next(a)}$\n",
        "    - если $d == \\text{True}$, выйти из цикла"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MRPZACJt7vG8"
      },
      "outputs": [],
      "source": [
        "def run_episode(mdp):\n",
        "    states, actions, T, R = mdp\n",
        "    agent = Agent(actions)\n",
        "\n",
        "    s = 0\n",
        "    tau = []\n",
        "    for _ in range(5):\n",
        "        a = agent.act(s)\n",
        "        s_next = T[(s, a)]\n",
        "        r = R[(s, a)]\n",
        "\n",
        "        tau.append((s, a, r))\n",
        "        s = s_next\n",
        "\n",
        "    return tau\n",
        "\n",
        "run_episode(A_mdp)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wbGEr8kl7xnS"
      },
      "source": [
        "Termination — означает окончание эпизода, когда достигнуто терминальное состояние. Является частью задания среды.\n",
        "\n",
        "Truncation — означает окончание эпизода, когда достигнут лимит по числу шагов (=времени). Обычно является внешне заданным параметром для удобства обучения.\n",
        "\n",
        "Пока не будем вводить truncation, но поддержим termination: расширьте определение среды информацией о терминальных состояниях для всех описанных ранее сред. Сгенерируйте по несколько случайных траекторий для каждой среды."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lxHNM9kS74WW"
      },
      "source": [
        "### Return, expected return\n",
        "\n",
        "Наиболее важная метрика оценки качества работы агента: отдача.\n",
        "\n",
        "Отдача: $G(s_t) = \\sum_{i=t}^T r_i$\n",
        "\n",
        "Обычно также вводят параметр $\\gamma \\in [0, 1]$, дисконтирующий будущие вознаграждения. А еще, тк отдача может меняться от запуска к запуску благодаря вероятностным процессам, нас интересует отдача в среднем — ожидаемая отдача:\n",
        "\n",
        "$$\\hat{G}(s_t) = \\mathbb{E} [ \\sum_{i=t}^T \\gamma^{i-t} r_i ]$$\n",
        "\n",
        "Именно ее и оптимизируют в RL.\n",
        "\n",
        "Давайте научимся считать отдачу для состояний по траектории и считать среднюю отдачу."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "scdThsNA8T2B"
      },
      "outputs": [],
      "source": [
        "# Функция для подсчета отдачи (return) по траектории\n",
        "def calculate_return(trajectory, gamma=1.0, start_idx=0):\n",
        "    \"\"\"\n",
        "    Вычисляет отдачу G(s_t) = sum_{i=t}^T gamma^{i-t} * r_i\n",
        "    \n",
        "    Parameters:\n",
        "    trajectory: список кортежей (s, a, r) или (s, a, r, s_next)\n",
        "    gamma: коэффициент дисконтирования\n",
        "    start_idx: индекс начала подсчета отдачи\n",
        "    \n",
        "    Returns:\n",
        "    return_value: отдача с позиции start_idx\n",
        "    \"\"\"\n",
        "    rewards = [r for s, a, r in trajectory[start_idx:]]\n",
        "    G = 0.0\n",
        "    for i, r in enumerate(rewards):\n",
        "        G += (gamma ** i) * r\n",
        "    return G\n",
        "\n",
        "# Функция для подсчета отдачи для каждого состояния в траектории\n",
        "def calculate_returns_for_states(trajectory, gamma=1.0):\n",
        "    \"\"\"\n",
        "    Вычисляет отдачу для каждого состояния в траектории\n",
        "    Returns: список отдач G(s_t) для каждого t\n",
        "    \"\"\"\n",
        "    returns = []\n",
        "    for t in range(len(trajectory)):\n",
        "        G_t = calculate_return(trajectory, gamma=gamma, start_idx=t)\n",
        "        returns.append(G_t)\n",
        "    return returns\n",
        "\n",
        "# Функция для подсчета средней отдачи (expected return)\n",
        "def calculate_expected_return(mdp, policy_fn, num_episodes=1000, gamma=1.0):\n",
        "    \"\"\"\n",
        "    Вычисляет среднюю отдачу за num_episodes эпизодов\n",
        "    \n",
        "    Parameters:\n",
        "    mdp: кортеж (states, actions, T, R) или (states, actions, T)\n",
        "    policy_fn: функция стратегии policy_fn(state, actions) -> action\n",
        "    num_episodes: количество эпизодов для усреднения\n",
        "    gamma: коэффициент дисконтирования\n",
        "    \n",
        "    Returns:\n",
        "    expected_return: средняя отдача\n",
        "    \"\"\"\n",
        "    states, actions, T = mdp[:3]\n",
        "    R = mdp[3] if len(mdp) > 3 else {}\n",
        "    \n",
        "    total_returns = []\n",
        "    \n",
        "    for episode in range(num_episodes):\n",
        "        s = 0  # начальное состояние\n",
        "        trajectory = []\n",
        "        max_steps = 100\n",
        "        step = 0\n",
        "        \n",
        "        while step < max_steps:\n",
        "            a = policy_fn(s, actions)\n",
        "            s_next = T[(s, a)]\n",
        "            r = R.get((s, a), 0.0)  # если R не задана, используем 0\n",
        "            \n",
        "            trajectory.append((s, a, r))\n",
        "            \n",
        "            # Проверка на терминальное состояние (если есть)\n",
        "            if s_next == s and step > 0:  # зацикливание может означать терминальное состояние\n",
        "                break\n",
        "                \n",
        "            s = s_next\n",
        "            step += 1\n",
        "        \n",
        "        if len(trajectory) > 0:\n",
        "            G = calculate_return(trajectory, gamma=gamma)\n",
        "            total_returns.append(G)\n",
        "    \n",
        "    return np.mean(total_returns) if total_returns else 0.0\n",
        "\n",
        "# Пример использования\n",
        "print(\"Пример траектории из MDP A:\")\n",
        "trajectory_example = run_episode(A_mdp)\n",
        "print(f\"Траектория: {trajectory_example}\")\n",
        "\n",
        "# Отдача для каждого состояния\n",
        "returns = calculate_returns_for_states(trajectory_example, gamma=1.0)\n",
        "print(f\"\\nОтдача для каждого состояния:\")\n",
        "for t, (s, a, r) in enumerate(trajectory_example):\n",
        "    print(f\"G(s_{t}) = {returns[t]:.2f}\")\n",
        "\n",
        "# Средняя отдача\n",
        "expected_G = calculate_expected_return(\n",
        "    A_mdp, \n",
        "    lambda s, acts: policy_random(s, acts), \n",
        "    num_episodes=1000,\n",
        "    gamma=1.0\n",
        ")\n",
        "print(f\"\\nСредняя отдача (1000 эпизодов): {expected_G:.4f}\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
