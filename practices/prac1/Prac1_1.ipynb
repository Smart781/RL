{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k4NMHBq16Y_g"
   },
   "source": [
    "# RL basics\n",
    "\n",
    "Термины и понятия:\n",
    "\n",
    "- агент/среда\n",
    "- наблюдение $o$ / состояние $s$\n",
    "- действие $a$, стратегия $\\pi: \\pi(s) \\rightarrow a$ функция перехода $T: T(s, a) \\rightarrow s'$\n",
    "- вознаграждение $r$, ф-я вознаграждений $R: R(s, a) \\rightarrow r$\n",
    "- цикл взаимодействия, траектория $\\tau: (s_0, a_0, r_0, s_1, a_1, r_1, ..., s_T, a_T, r_T)$, эпизод\n",
    "- отдача $G$, подсчет отдачи, средняя[/ожидаемая] отдача $\\mathbb{E}[G]$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "MoNP7Wdn6aP0"
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    import google.colab\n",
    "    COLAB = True\n",
    "except ModuleNotFoundError:\n",
    "    COLAB = False\n",
    "    pass\n",
    "\n",
    "if COLAB:\n",
    "    !pip -q install \"gymnasium[classic-control, atari, accept-rom-license]\"\n",
    "    !pip -q install piglet\n",
    "    !pip -q install imageio_ffmpeg\n",
    "    !pip -q install moviepy==1.0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Looking in indexes: https://pypi.org/simple/\n",
      "Requirement already satisfied: gymnasium in /home/trueuser/.local/lib/python3.7/site-packages (0.28.1)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in /home/trueuser/.local/lib/python3.7/site-packages (from gymnasium) (2.2.1)\n",
      "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.7/dist-packages (from gymnasium) (4.7.1)\n",
      "Requirement already satisfied: jax-jumpy>=1.0.0 in /home/trueuser/.local/lib/python3.7/site-packages (from gymnasium) (1.0.0)\n",
      "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.7/dist-packages (from gymnasium) (1.21.6)\n",
      "Requirement already satisfied: importlib-metadata>=4.8.0 in /usr/local/lib/python3.7/dist-packages (from gymnasium) (6.7.0)\n",
      "Requirement already satisfied: farama-notifications>=0.0.1 in /home/trueuser/.local/lib/python3.7/site-packages (from gymnasium) (0.0.4)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.8.0->gymnasium) (3.15.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install gymnasium --no-cache-dir -i https://pypi.org/simple/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "9JPaLF5v6esZ"
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "import io\n",
    "import base64\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "from IPython import display as ipythondisplay\n",
    "from IPython.display import HTML\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OGAoJeNF6hJO"
   },
   "source": [
    "## Agent, environment\n",
    "\n",
    "<img src=https://gymnasium.farama.org/_images/lunar_lander.gif caption=\"lunar lander\" width=\"150\" height=\"50\"><img src=https://gymnasium.farama.org/_images/mountain_car.gif caption=\"mountain car\" width=\"150\" height=\"50\">\n",
    "<img src=https://gymnasium.farama.org/_images/cliff_walking.gif caption=\"cliff walking\" width=\"300\" height=\"50\">\n",
    "<img src=https://ale.farama.org/_images/montezuma_revenge.gif caption=\"montezuma revenge\" width=\"150\" height=\"100\">\n",
    "<img src=https://github.com/danijar/crafter/raw/main/media/video.gif caption=\"crafter\" width=\"150\" height=\"100\">\n",
    "<img src=https://camo.githubusercontent.com/6df2ca438d8fe8aa7a132b859315147818c54af608f8609320c3c20e938acf48/68747470733a2f2f6d656469612e67697068792e636f6d2f6d656469612f344e78376759694d394e44724d724d616f372f67697068792e676966 caption=\"malmo minecraft\" width=\"150\" height=\"100\">\n",
    "<img src=https://images.ctfassets.net/kftzwdyauwt9/e0c0947f-1a44-4528-4a41450a9f0a/2d0e85871d58d02dbe01b2469d693d4a/table-03.gif caption=\"roboschool\" width=\"150\" height=\"100\">\n",
    "<img src=https://raw.githubusercontent.com/Tviskaron/mipt/master/2019/RL/02/mdp.png caption=\"Марковский процесс принятия решений\" width=\"150\" height=\"100\">\n",
    "<img src=https://minigrid.farama.org/_images/DoorKeyEnv.gif caption=\"minigrid\" width=\"120\" height=\"120\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dcyLKga76mA_"
   },
   "source": [
    "## Observation, state\n",
    "\n",
    "TODO:\n",
    "- добавить примеры наблюдений/состояний (числа, векторы, картинки)\n",
    "- интуитивное объяснение различия, положить пока, что наблюдение = состояние\n",
    "- пространство состояний\n",
    "\n",
    "\n",
    "В каждый момент времени среда имеет некоторое внутреннее состояние. Здесь слово \"состояние\" я употребил скорее в интуитивном понимании, чтобы обозначить, что среда изменчива (иначе какой смысл с ней взаимодействовать, если ничего не меняется). В обучении с подкреплением под термином состояние $s$ (или $s_t$, где $t$ — текущее время) подразумевают либо абстрактно информацию о \"состоянии\" среды, либо ее явное представление в виде данных, достаточные для полного описания \"состояния\". *NB: Здесь можно провести аналогию с компьютерными играми — файл сохранения игры как раз содержит информацию о \"состоянии\" мира игры, чтобы в будущем можно было продолжить с текущей точки, так что данные этого файла в целом можно с некоторой натяжкой считать состоянием (с натяжкой, потому что редко когда в сложных играх файлы сохранения содержат прямо вот всю информацию, так что после перезагрузки вы получите не совсем точную копию). При этом обычно подразумевается, что состояние не содержит в себе ничего лишнего, то есть это **минимальный** набор информации.*\n",
    "\n",
    "Наблюдением $o$ называют то, что агент \"видит\" о текущем состоянии среды. Это не обязательно зрение, а вообще вся доступная ему информация (условно, со всех его органов чувств).\n",
    "\n",
    "В общем случае наблюдение: кортеж/словарь многомерных векторов чисел."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "ypHv9w6i6pcX"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4,)\n",
      "(2,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/trueuser/.local/lib/python3.7/site-packages/gymnasium/envs/registration.py:524: DeprecationWarning: \u001b[33mWARN: The environment CartPole-v0 is out of date. You should consider upgrading to version `v1`.\u001b[0m\n",
      "  f\"The environment {env_name} is out of date. You should consider \"\n"
     ]
    }
   ],
   "source": [
    "print(gym.make(\"CartPole-v0\").reset()[0].shape)\n",
    "print(gym.make(\"MountainCar-v0\").reset()[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.7.5\n"
     ]
    }
   ],
   "source": [
    "!python3 --version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GabuCLcJ67lb"
   },
   "source": [
    "## Action, policy, transition function\n",
    "\n",
    "Рассмотрим следующие MDP:\n",
    "\n",
    "- A: <img src=https://i.ibb.co/mrCMVZLQ/mdp-a.png caption=\"A\" width=\"400\" height=\"100\">\n",
    "- B: <img src=https://i.ibb.co/GQ2tVtjC/mdp-b.png caption=\"B\" width=\"400\" height=\"100\">\n",
    "\n",
    "Links to all:\n",
    "[A](https://i.ibb.co/mrCMVZLQ/mdp-a.png)\n",
    "[B](https://i.ibb.co/GQ2tVtjC/mdp-b.png)\n",
    "[C](https://i.ibb.co/Jj9LYHjP/mdp-c.png)\n",
    "[D](https://i.ibb.co/Y47Mr83b/mdp-d.png)\n",
    "[E](https://i.ibb.co/Kjt1Xhmf/mdp-e.png)\n",
    "\n",
    "Давайте явно запишем пространства состояний $S$ и действий $A$, а также функцию перехода $T$ среды."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "4XpqNc_o6_CS"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "states={0, 1, 2} | actions={0}\n",
      "Transition function T={(0, 0): 1, (1, 0): 2, (2, 0): 2}\n"
     ]
    }
   ],
   "source": [
    "states = set(range(3))\n",
    "actions = set(range(1))\n",
    "\n",
    "print('states=' + str(states) + ' | actions=' + str(actions))\n",
    "\n",
    "T = {\n",
    "    (0, 0): 1,\n",
    "    (1, 0): 2,\n",
    "    (2, 0): 2\n",
    "}\n",
    "\n",
    "print('Transition function T=' + str(T))\n",
    "\n",
    "A_mdp = states, actions, T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KlqB4WcZ7CDK"
   },
   "source": [
    "Попробуйте записать функцию перехода в матричном виде:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "jmg8hnng7EnJ"
   },
   "outputs": [],
   "source": [
    "def transition_function_to_matrix(T):\n",
    "    max_state = 0\n",
    "    for key, next_state in T.items():\n",
    "        s = key[0]\n",
    "        if s > max_state:\n",
    "            max_state = s\n",
    "        if next_state > max_state:\n",
    "            max_state = next_state\n",
    "    \n",
    "    n_states = max_state + 1\n",
    "\n",
    "    counts = []\n",
    "    for i in range(n_states):\n",
    "        row = []\n",
    "        for j in range(n_states):\n",
    "            row.append(0.0)\n",
    "        counts.append(row)\n",
    "\n",
    "    actions_per_state = []\n",
    "    for i in range(n_states):\n",
    "        actions_per_state.append(0)\n",
    "\n",
    "    for key, next_state in T.items():\n",
    "        s = key[0]\n",
    "        counts[s][next_state] += 1\n",
    "        actions_per_state[s] += 1\n",
    "\n",
    "    P = []\n",
    "    for i in range(n_states):\n",
    "        row = []\n",
    "        for j in range(n_states):\n",
    "            row.append(0.0)\n",
    "        P.append(row)\n",
    "    for s in range(n_states):\n",
    "        if actions_per_state[s] > 0:\n",
    "            for s_next in range(n_states):\n",
    "                P[s][s_next] = counts[s][s_next] / actions_per_state[s]\n",
    "        else:\n",
    "            P[s][s] = 1.0\n",
    "    P = np.array(P)\n",
    "\n",
    "    return P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "P = transition_function_to_matrix(T)\n",
    "P"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_FPshg_07G0R"
   },
   "source": [
    "Как получить вероятность нахождения агента в состоянии (1) через N шагов? Что происходит с вероятностями нахождения в состояниях при $N \\rightarrow \\infty$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "9pwc4Atn7IAf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Кол-во шагов: 1\n",
      "Вероятность: [0. 1. 0.]\n",
      "\n",
      "Кол-во шагов: 100\n",
      "Вероятность: [0. 0. 1.]\n",
      "\n",
      "Кол-во шагов: 1000\n",
      "Вероятность: [0. 0. 1.]\n",
      "\n",
      "Кол-во шагов: 5000\n",
      "Вероятность: [0. 0. 1.]\n",
      "\n",
      "Кол-во шагов: 50000\n",
      "Вероятность: [0. 0. 1.]\n",
      "\n",
      "Кол-во шагов: 100000\n",
      "Вероятность: [0. 0. 1.]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#A\n",
    "P_start = [1, 0, 0]\n",
    "\n",
    "def P_n(state, steps, matrix, start):\n",
    "    ans = np.dot(start, np.linalg.matrix_power(matrix, steps))\n",
    "    \n",
    "    return str('Кол-во шагов: ' + str(steps) + '\\n' + 'Вероятность: ' + str(ans) + '\\n')\n",
    "    \n",
    "for n in [1, 100, 1000, 5000, 50000, 100000]:\n",
    "    print(P_n(1, n, P, P_start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.33333333 0.33333333 0.33333333]\n",
      " [0.         1.         0.         0.        ]\n",
      " [0.         0.         1.         0.        ]\n",
      " [0.         0.         0.         1.        ]]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#B\n",
    "\n",
    "T_B = {} \n",
    "T_B[(0, 0)] = 1  \n",
    "T_B[(0, 1)] = 2 \n",
    "T_B[(0, 2)] = 3 \n",
    "T_B[(1, 0)] = 1\n",
    "T_B[(1, 1)] = 1\n",
    "T_B[(1, 2)] = 1\n",
    "T_B[(2, 0)] = 2\n",
    "T_B[(2, 1)] = 2\n",
    "T_B[(2, 2)] = 2\n",
    "T_B[(3, 0)] = 3\n",
    "T_B[(3, 1)] = 3\n",
    "T_B[(3, 2)] = 3\n",
    "\n",
    "\n",
    "states = set()\n",
    "actions = set()\n",
    "\n",
    "for i in range(4):\n",
    "    states.add(i)\n",
    "    \n",
    "for i in range(3):\n",
    "    actions.add(i)\n",
    "B = states, actions, T_B\n",
    "\n",
    "P_B = transition_function_to_matrix(T_B)\n",
    "\n",
    "print(str(P_B) + '\\n\\n')\n",
    "\n",
    "P_0_B = [1, 0, 0, 0]\n",
    "\n",
    "steps_to_check = [1, 100, 1000, 5000, 50000, 100000]\n",
    "\n",
    "for n in steps_to_check:\n",
    "    P_n(1, n, P_B, P_0_B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p52R04np7Ku0"
   },
   "source": [
    "Задайте еще несколько MDP:\n",
    "\n",
    "- C: <img src=https://i.ibb.co/Jj9LYHjP/mdp-c.png caption=\"C\" width=\"400\" height=\"100\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "JSdJ9ZsI7Nfw"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.  0.5 0.5 0. ]\n",
      " [0.  0.5 0.  0.5]\n",
      " [0.  0.  0.5 0.5]\n",
      " [0.  0.  0.  1. ]]\n"
     ]
    }
   ],
   "source": [
    "#C\n",
    "\n",
    "T_C = {}\n",
    "T_C[(0, 0)] = 1\n",
    "T_C[(0, 1)] = 2\n",
    "T_C[(1, 0)] = 1\n",
    "T_C[(1, 1)] = 3\n",
    "T_C[(2, 0)] = 3\n",
    "T_C[(2, 1)] = 2\n",
    "T_C[(3, 0)] = 3\n",
    "T_C[(3, 1)] = 3\n",
    "\n",
    "states = set()\n",
    "actions = set()\n",
    "\n",
    "for i in range(4):\n",
    "    states.add(i)\n",
    "for i in range(2):\n",
    "    actions.add(i)\n",
    "\n",
    "my_mdp_c = states, actions, T_C\n",
    "\n",
    "trans_matrix_c = transition_function_to_matrix(T_C)\n",
    "\n",
    "print(trans_matrix_c)\n",
    "\n",
    "initial_p_c = [1, 0, 0, 0]\n",
    "\n",
    "steps_to_check = [1, 100, 1000, 5000, 50000, 100000]\n",
    "\n",
    "# Цикл: для каждого количества шагов\n",
    "for n in steps_to_check:\n",
    "    P_n(1, n, trans_matrix_c, initial_p_c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AiU5X4DH7TaG"
   },
   "source": [
    "Давайте попробуем задать двух агентов: случайного и оптимального (для каждой среды свой)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "It2waXJi7WWN"
   },
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, actions):\n",
    "        self.rng = np.random.default_rng()\n",
    "        self.actions = np.array(list(actions))\n",
    "\n",
    "    def act(self, state):\n",
    "        return self.rng.integers(len(self.actions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VH6uo4EP7ZqB"
   },
   "source": [
    "В качестве дополнения, запишите стратегию агента"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "t68c_r-W7asH"
   },
   "outputs": [],
   "source": [
    "class AgentA:\n",
    "    def __init__(self, all_actions):\n",
    "        self.act_list = list(all_actions)\n",
    "\n",
    "    def choose_action(self, current_state):\n",
    "        return 0\n",
    "\n",
    "class AgentB:\n",
    "    def __init__(self, all_actions):\n",
    "        self.act_list = list(all_actions)\n",
    "        self.n_act = len(self.act_list) \n",
    "\n",
    "    def choose_action(self, current_state):\n",
    "        rand_num = np.random.randint(0, self.n_act)\n",
    "        return rand_num\n",
    "\n",
    "class AgentC:\n",
    "    def __init__(self, all_actions):\n",
    "        self.act_list = list(all_actions)\n",
    "        self.n_act = len(self.act_list)\n",
    "\n",
    "    def choose_action(self, current_state):\n",
    "        action = current_state % 2\n",
    "        return int(action)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XGx2-KeH7lL4"
   },
   "source": [
    "## Reward, reward function\n",
    "\n",
    "Теперь добавим произвольную функцию вознаграждения. Например, для A:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "fk7umEnA7oFv"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{(0, 0): -0.1, (1, 0): 1.0, (2, 0): 0.0}\n",
      "({0, 1, 2}, {0}, {(0, 0): 1, (1, 0): 2, (2, 0): 2}, {(0, 0): -0.1, (1, 0): 1.0, (2, 0): 0.0})\n"
     ]
    }
   ],
   "source": [
    "R = {\n",
    "    (0, 0): -0.1,\n",
    "    (1, 0): 1.0,\n",
    "    (2, 0): 0.0\n",
    "}\n",
    "print(R)\n",
    "\n",
    "A_mdp = *A_mdp, R\n",
    "print(A_mdp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j92TZ1l67rVh"
   },
   "source": [
    "## Interaction loop, trajectory, termination, truncation, episode\n",
    "\n",
    "Общий цикл взаимодействия в рамках эпизода:\n",
    "1. Инициализировать среду: $s \\leftarrow \\text{env.init()}$\n",
    "2. Цикл:\n",
    "    - выбрать действие: $a \\leftarrow \\pi(s)$\n",
    "    - получить ответ от среды: $s, r, d \\leftarrow \\text{env.next(a)}$\n",
    "    - если $d == \\text{True}$, выйти из цикла"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "({0, 1, 2, 3}, {0, 1, 2}, {(0, 0): 1, (0, 1): 2, (0, 2): 3, (1, 0): 1, (1, 1): 1, (1, 2): 1, (2, 0): 2, (2, 1): 2, (2, 2): 2, (3, 0): 3, (3, 1): 3, (3, 2): 3}, {(0, 0): 0.05, (0, 1): 0.1, (0, 2): 0.15, (1, 0): 0.0, (1, 1): 0.0, (1, 2): 0.0, (2, 0): 0.0, (2, 1): 0.0, (2, 2): 0.0, (3, 0): 0.0, (3, 1): 0.0, (3, 2): 0.0})\n",
      "\n",
      "{(0, 0): 0.05, (0, 1): 0.15, (1, 0): -0.05, (1, 1): 0.15, (2, 0): 0.1, (2, 1): -0.15, (3, 0): 0.0, (3, 1): 0.0}\n",
      "({0, 1, 2, 3}, {0, 1}, {(0, 0): 1, (0, 1): 2, (1, 0): 1, (1, 1): 3, (2, 0): 3, (2, 1): 2, (3, 0): 3, (3, 1): 3}, {(0, 0): 0.05, (0, 1): 0.15, (1, 0): -0.05, (1, 1): 0.15, (2, 0): 0.1, (2, 1): -0.15, (3, 0): 0.0, (3, 1): 0.0})\n"
     ]
    }
   ],
   "source": [
    "R_B = {\n",
    "    (0, 0): 0.05,\n",
    "    (0, 1): 0.1,\n",
    "    (0, 2): 0.15,\n",
    "    (1, 0): 0.0,\n",
    "    (1, 1): 0.0,\n",
    "    (1, 2): 0.0,\n",
    "    (2, 0): 0.0,\n",
    "    (2, 1): 0.0,\n",
    "    (2, 2): 0.0,\n",
    "    (3, 0): 0.0,\n",
    "    (3, 1): 0.0,\n",
    "    (3, 2): 0.0\n",
    "}\n",
    "\n",
    "\n",
    "B = *B, R_B\n",
    "print(B)\n",
    "print()\n",
    "\n",
    "R_C = {\n",
    "    (0, 0): 0.05,\n",
    "    (0, 1): 0.15,\n",
    "    (1, 0): -0.05,\n",
    "    (1, 1): 0.15,\n",
    "    (2, 0): 0.1,\n",
    "    (2, 1): -0.15,\n",
    "    (3, 0): 0.0,\n",
    "    (3, 1): 0.0\n",
    "}\n",
    "\n",
    "print(R_C)\n",
    "my_mdp_c = *my_mdp_c, R_C\n",
    "print(my_mdp_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "MRPZACJt7vG8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 0, -0.1), (1, 0, 1.0), (2, 0, 0.0), (2, 0, 0.0), (2, 0, 0.0)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def run_episode(mdp):\n",
    "    states, actions, T, R = mdp\n",
    "    agent = Agent(actions)\n",
    "\n",
    "    s = 0\n",
    "    tau = []\n",
    "    for _ in range(5):\n",
    "        a = agent.act(s)\n",
    "        s_next = T[(s, a)]\n",
    "        r = R[(s, a)]\n",
    "\n",
    "        tau.append((s, a, r))\n",
    "        s = s_next\n",
    "\n",
    "    return tau\n",
    "\n",
    "run_episode(A_mdp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wbGEr8kl7xnS"
   },
   "source": [
    "Termination — означает окончание эпизода, когда достигнуто терминальное состояние. Является частью задания среды.\n",
    "\n",
    "Truncation — означает окончание эпизода, когда достигнут лимит по числу шагов (=времени). Обычно является внешне заданным параметром для удобства обучения.\n",
    "\n",
    "Пока не будем вводить truncation, но поддержим termination: расширьте определение среды информацией о терминальных состояниях для всех описанных ранее сред. Сгенерируйте по несколько случайных траекторий для каждой среды."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lxHNM9kS74WW"
   },
   "source": [
    "### Return, expected return\n",
    "\n",
    "Наиболее важная метрика оценки качества работы агента: отдача.\n",
    "\n",
    "Отдача: $G(s_t) = \\sum_{i=t}^T r_i$\n",
    "\n",
    "Обычно также вводят параметр $\\gamma \\in [0, 1]$, дисконтирующий будущие вознаграждения. А еще, тк отдача может меняться от запуска к запуску благодаря вероятностным процессам, нас интересует отдача в среднем — ожидаемая отдача:\n",
    "\n",
    "$$\\hat{G}(s_t) = \\mathbb{E} [ \\sum_{i=t}^T \\gamma^{i-t} r_i ]$$\n",
    "\n",
    "Именно ее и оптимизируют в RL.\n",
    "\n",
    "Давайте научимся считать отдачу для состояний по траектории и считать среднюю отдачу."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
